{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA T1200 Laptop GPU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39181"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./the_things.txt\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9275"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(raw_text)\n",
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(Dataset):\n",
    "    def __init__(self, token_ids, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            self.input_ids.append(torch.tensor(token_ids[i : i + max_length]))\n",
    "            self.target_ids.append(torch.tensor(token_ids[i + 1 : i + 1 + max_length]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ds = BookDataset(token_ids, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_loader = DataLoader(book_ds, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": tokenizer.n_vocab,\n",
    "    \"context_length\": 1024,\n",
    "    \"embed_dim\": 768,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 12,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim, out_dim, context_length, dropout, num_heads, qkv_bias=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert out_dim % num_heads == 0, \"out_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_dim // num_heads\n",
    "\n",
    "        self.query_weight = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
    "        self.key_weight = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
    "        self.value_weight = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
    "\n",
    "        self.output_projection = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, in_dim = x.shape\n",
    "\n",
    "        keys = self.key_weight(x)\n",
    "        queries = self.query_weight(x)\n",
    "        values = self.value_weight(x)\n",
    "\n",
    "        # Split the weights to have qkv for each head\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attention_scores = attention_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector = (attention_weights @ values).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(\n",
    "            batch_size, num_tokens, self.out_dim\n",
    "        )\n",
    "        context_vector = self.output_projection(context_vector)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.epsilon = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_variance = x.var(dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / torch.sqrt(x_variance + self.epsilon)\n",
    "\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
    "                    * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            in_dim=config[\"embed_dim\"],\n",
    "            out_dim=config[\"embed_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            dropout=config[\"dropout_rate\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            qkv_bias=config[\"qkv_bias\"],\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(config[\"embed_dim\"])\n",
    "\n",
    "        self.norm1 = LayerNorm(config['embed_dim'])\n",
    "        self.norm2 = LayerNorm(config['embed_dim'])\n",
    "\n",
    "        self.shortcut_dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.shortcut_dropout(x)\n",
    "        x += shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.shortcut_dropout\n",
    "        x += shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(config['context_length'], config['embed_dim'])\n",
    "        \n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "        self.transformers = nn.Sequential(*[Transformer(config) for _ in range(config['num_layers'])])\n",
    "\n",
    "        self.norm = LayerNorm(config['embed_dim'])\n",
    "        self.out_head = nn.Linear(config['embed_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(torch.arange(seq_len, device=x.device))\n",
    "\n",
    "        x = token_embeds + position_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformers(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
