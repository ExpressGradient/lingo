{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\SaiPraneethDiddigam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\SaiPraneethDiddigam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1919, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\SaiPraneethDiddigam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\SaiPraneethDiddigam\\AppData\\Local\\Temp\\ipykernel_17244\\2449057751.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\SaiPraneethDiddigam\\softwares\\personal\\lingo\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA T1200 Laptop GPU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39181"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./the_things.txt\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9275"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(raw_text)\n",
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(Dataset):\n",
    "    def __init__(self, token_ids, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            self.input_ids.append(torch.tensor(token_ids[i : i + max_length]))\n",
    "            self.target_ids.append(torch.tensor(token_ids[i + 1 : i + 1 + max_length]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ds = BookDataset(token_ids, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_loader = DataLoader(book_ds, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": tokenizer.n_vocab,\n",
    "    \"context_length\": 1024,\n",
    "    \"embed_dim\": 768,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 12,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim, out_dim, context_length, dropout, num_heads, qkv_bias=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert out_dim % num_heads == 0, \"out_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_dim // num_heads\n",
    "\n",
    "        self.query_weight = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
    "        self.key_weight = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
    "        self.value_weight = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
    "\n",
    "        self.output_projection = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, in_dim = x.shape\n",
    "\n",
    "        keys = self.key_weight(x)\n",
    "        queries = self.query_weight(x)\n",
    "        values = self.value_weight(x)\n",
    "\n",
    "        # Split the weights to have qkv for each head\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attention_scores = attention_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector = (attention_weights @ values).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(\n",
    "            batch_size, num_tokens, self.out_dim\n",
    "        )\n",
    "        context_vector = self.output_projection(context_vector)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.epsilon = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_variance = x.var(dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / torch.sqrt(x_variance + self.epsilon)\n",
    "\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
    "                    * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            in_dim=config[\"embed_dim\"],\n",
    "            out_dim=config[\"embed_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            dropout=config[\"dropout_rate\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            qkv_bias=config[\"qkv_bias\"],\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(config[\"embed_dim\"])\n",
    "\n",
    "        self.norm1 = LayerNorm(config['embed_dim'])\n",
    "        self.norm2 = LayerNorm(config['embed_dim'])\n",
    "\n",
    "        self.shortcut_dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.shortcut_dropout(x)\n",
    "        x += shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.shortcut_dropout\n",
    "        x += shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(config['context_length'], config['embed_dim'])\n",
    "        \n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "        self.transformers = nn.Sequential(*[Transformer(config) for _ in range(config['num_layers'])])\n",
    "\n",
    "        self.norm = LayerNorm(config['embed_dim'])\n",
    "        self.out_head = nn.Linear(config['embed_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(torch.arange(seq_len, device=x.device))\n",
    "\n",
    "        x = token_embeds + position_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformers(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
